# https://github.com/pgvector/pgvector
# The Postgres function uses Cosine Distance to find similar videos.
# <=> - Cosine Distance. Cosine distance (or its complement, cosine similarity) is widely recommended for LLM embeddings.
# This is because embeddings generated by LLMs (like GPT, BERT, etc.) typically represent semantic meaning in a
# igh-dimensional space. Cosine distance measures the cosine of the angle between two vectors, which effectively captures
# the directional similarity between them, irrespective of their magnitude.

import os
import logging
from typing import Any, AsyncGenerator, List
from contextlib import asynccontextmanager
import httpx

from dotenv import load_dotenv
import asyncpg
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from ollama import Client

logging.basicConfig(level=logging.INFO)  # You can set the desired logging level

# Load environment variables from .env file
load_dotenv()


OLLAMA_EMBEDDING_ENDPOINT = os.getenv("OLLAMA_EMBEDDING_ENDPOINT")
POSTGRES_CONNECTION_STRING = os.getenv("POSTGRES_CONNECTION_STRING")
OLLAMA_EMBEDDING_MODEL = os.getenv("OLLAMA_EMBEDDING_MODEL")

custom_client = Client(host=OLLAMA_EMBEDDING_ENDPOINT, timeout=10)
# Create a persistent client instance
client = httpx.AsyncClient(timeout=10.0, limits=httpx.Limits(max_connections=100, max_keepalive_connections=20))


class PromptRequest(BaseModel):
    prompt: str = "What is the best way to learn about cognitive services."
    distance: float = Field(default=0.4, ge=0.0, le=1.0)
    limit: int = Field(default=4, ge=1, le=100)


@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncGenerator[Any, Any]:
    app.state.db_pool = await asyncpg.create_pool(dsn=POSTGRES_CONNECTION_STRING, min_size=1, max_size=10)
    try:
        yield
    finally:
        await app.state.db_pool.close()


app = FastAPI(lifespan=lifespan)

async def get_vector_data_async(prompt: str) -> List[float]:
    try:
        response = await client.post(
            OLLAMA_EMBEDDING_ENDPOINT,
            json={"model": OLLAMA_EMBEDDING_MODEL, "input": prompt},
            timeout=10.0
        )
        response.raise_for_status()
        embedding_result = response.json()
        return embedding_result["embeddings"][0]
    except httpx.TimeoutException as e:
        logging.error(f"Timeout error occurred: {e}")
        raise
    except httpx.RequestError as e:
        logging.error(f"Request error occurred: {e}")
        raise
    except Exception as e:
        logging.error(f"An error occurred while fetching data: {e}")
        raise


@app.post("/get-videos/")
async def get_videos(request: PromptRequest) -> list:
    if not request.prompt:
        raise HTTPException(status_code=400, detail="Prompt cannot be empty")

    async with app.state.db_pool.acquire() as connection:
        try:
            vector = await get_vector_data_async(request.prompt)
            vector_string = f"[{', '.join(map(str, vector))}]"

            select_query = "SELECT * FROM public.get_similar_videos($1, $2, $3)"
            results = await connection.fetch(select_query, vector_string, request.distance, request.limit)

            return [
                {
                    "title": result["title"],
                    "distance": result["distance"],
                    "youtube_link": f'https://youtu.be/{result["videoid"]}&t={result["seconds"]}',
                    "text": result["text"],
                }
                for result in results
            ]

        except asyncpg.exceptions.PostgresError as e:
            logging.error(f"An error occurred while executing the Postgres query: {e}")
            raise HTTPException(status_code=500, detail="Internal server error") from e

        except Exception as e:
            logging.error(f"An error occurred while fetching data: {e}")
            raise HTTPException(status_code=500, detail="Internal server error") from e


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
